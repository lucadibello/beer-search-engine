\section{Indexing}

To build the index based on the collected data, we used the \href{https://github.com/terrier-org/pyterrier}{PyTerrier} library, which is a Python library for the Terrier IR platform. The library provides a set of tools to build and evaluate IR systems, and it's built on top of the Terrier platform, which is a Java-based open-source IR platform.

After the completion of the crawling of the data, we realized we needed a set of strings to be able to create an index, which we did not have yet ready for index creation as our dataset was composed of JSON objects.

Our decision was thus to move on to put together all the data for each record into a single string by recursively taking all the values of the various fields and concatenating them with a blank space in between. We decided to go with this structure as indexers usually have to deal with plain text, and thus it's reasonable to unify the data in this way as that's how data would usually look after scraping them.

After this step, we moved on the actual index creation. To address this task, we leveraged the \texttt{IterDictIndexer} class, which allows to build an index from a dictionary of textual documents. The index has been built using the following settings:

\begin{itemize}
  \item \textbf{Retrieval model}: BM25. This is a probabilistic retrieval model that ranks documents based on the query terms appearing in each document, learn more \href{https://en.wikipedia.org/wiki/Okapi_BM25}{here}.
  \item \textbf{Stemmer}: Porter stemmer. This stemmer is based on the original Porter stemmer algorithm, which is a widely used stemmer for the English language.
  \item \textbf{Stopwords}: PyTerrier default stopwords for English language.
  \item \textbf{Tokeniser}: PyTerrier default tokeniser for English language.
\end{itemize}
